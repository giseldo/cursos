import{_ as i,c as d,a2 as r,j as a,G as t,w as n,a as o,B as m,o as l}from"./chunks/framework.H8_ecXae.js";const p="/cursos/assets/fox.vlwafcYG.png",u="/cursos/assets/regressao.BU19LlJt.png",c="/cursos/assets/llmarch.C_qRo19l.png",g="/cursos/assets/linhadotempo.b4jBv9IK.png",k=JSON.parse('{"title":"Large Language Models: A Short Introduction","description":"","frontmatter":{"draft":true},"headers":[],"relativePath":"llm/index.md","filePath":"llm/index.md"}'),f={name:"llm/index.md"};function v(b,e,h,q,L,x){const s=m("center");return l(),d("div",null,[e[4]||(e[4]=r('<h1 id="large-language-models-a-short-introduction" tabindex="-1">Large Language Models: A Short Introduction <a class="header-anchor" href="#large-language-models-a-short-introduction" aria-label="Permalink to &quot;Large Language Models: A Short Introduction&quot;">​</a></h1><h2 id="sobre" tabindex="-1">sobre <a class="header-anchor" href="#sobre" aria-label="Permalink to &quot;sobre&quot;">​</a></h2><p>Este é uma tradução para portugues brasileiro, com alguns ajustes e remoção de algumas partes, da postagem publicada no <a href="https://towardsdatascience.com/large-language-models-a-short-introduction-bb8366118ad0" target="_blank" rel="noreferrer">medium</a> de autoria de Carolina Bento com o título &quot;Large Language Models: A Short Introduction&quot;.</p><p>Neste artigo, daremos uma breve olhada no que são LLMs, por que eles são uma tecnologia extremamente interessante, por que eles são importantes.</p><p>Observação: neste artigo, usaremos Large Language Model, LLM e modelo de forma intercambiável.</p><h2 id="o-que-e-um-llm" tabindex="-1">O que é um LLM <a class="header-anchor" href="#o-que-e-um-llm" aria-label="Permalink to &quot;O que é um LLM&quot;">​</a></h2><p>Um <a href="https://en.wikipedia.org/wiki/Large_language_model" target="_blank" rel="noreferrer">modelo de linguagem grande</a>, normalmente chamado de LLM, é um modelo matemático que gera texto, como preencher a lacuna para a próxima palavra em uma frase.</p>',7)),e[5]||(e[5]=a("iframe",{width:"100%",height:"522",src:"https://www.youtube.com/embed/LPZh9BOjkQs",title:"Large Language Models explained briefly",frameborder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share",referrerpolicy:"strict-origin-when-cross-origin",allowfullscreen:""},null,-1)),e[6]||(e[6]=r('<p>Por exemplo, quando você o alimenta com a frase</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>The quick brown fox jumps over the lazy ____ ,</span></span></code></pre></div><p>ele não sabe exatamente que a próxima palavra é <code>dog</code>. O que o modelo produz em vez disso é uma lista de possíveis próximas palavras com suas probabilidades correspondentes de virem a seguir em uma frase que começa com essas palavras exatas.</p><p><img src="'+p+'" alt=""></p>',4)),t(s,null,{default:n(()=>e[0]||(e[0]=[a("small",null,[o("Exemplo de previsão da próxima palavra em uma frase. "),a("a",{href:"https://towardsdatascience.com/large-language-models-a-short-introduction-bb8366118ad0)"},"Fonte da Imagem"),o(".")],-1)])),_:1}),e[7]||(e[7]=a("p",null,"A razão pela qual os LLMs são tão bons em prever a próxima palavra em uma frase é porque eles são treinados com uma quantidade incrivelmente grande de texto, que normalmente é raspado da Internet.",-1)),e[8]||(e[8]=a("p",null,"Por outro lado, se você estiver construindo um LLM que seja específico para um domínio em particular, por exemplo, você está construindo um chatbot que poderia conversar com você como se fosse um personagem nas peças de Shakespeare , a internet certamente terá muitos trechos ou até mesmo suas obras completas, mas terá uma tonelada de outros textos que não são relevantes para a tarefa em questão. Neste caso, você alimentaria o LLM no contexto de Shakespeare apenas do chatbot, ou seja, todas as suas peças e sonetos.",-1)),e[9]||(e[9]=a("p",null,"Embora os LLMs sejam treinados com uma quantidade gigantesca de dados, não é isso que o Large in Large Language Models representa. Além do tamanho dos dados de treinamento, a outra grande quantidade nesses modelos é o número de parâmetros que eles têm, cada um com a possibilidade de ser ajustado, ou seja, sintonizado.",-1)),e[10]||(e[10]=a("p",null,"O modelo estatístico mais simples é a Regressão Linear Simples , com apenas dois parâmetros, a inclinação e a interceptação. E mesmo com apenas dois parâmetros, há algumas formas diferentes que a saída do modelo pode assumir.",-1)),e[11]||(e[11]=a("p",null,[a("img",{src:u,alt:"alt text"})],-1)),t(s,null,{default:n(()=>e[1]||(e[1]=[a("small",null,[o("Diferentes formas de uma regressão linear. "),a("a",{href:"https://towardsdatascience.com/large-language-models-a-short-introduction-bb8366118ad0)"},"Fonte da Imagem"),o(".")],-1)])),_:1}),e[12]||(e[12]=r('<p>Como comparação, quando o GPT-3 foi lançado em 2020, ele tinha 175B parâmetros, sim, bilhões![3] Enquanto o LLaMa, o LLM de código aberto do Meta, tinha vários modelos diferentes variando de 7B a 65B parâmetros quando foi lançado em 2023 .</p><p>Todos esses bilhões de parâmetros começam com valores aleatórios, no início do processo de treinamento, e é durante a parte de retropropagação da fase de treinamento que eles são continuamente ajustados e modificados.</p><p>Semelhante a qualquer outro modelo de Machine Learning, durante a fase de treinamento, a saída do modelo é comparada com o valor real esperado para a saída, a fim de calcular o erro. Quando ainda há espaço para melhorias, a Backpropagation garante que os parâmetros do modelo sejam ajustados de forma que o modelo possa prever valores com um pouco menos de erro na próxima vez.</p><p>Mas isso é apenas o que chamamos de pré-treinamento , onde o modelo se torna proficiente em prever a próxima palavra em uma frase.</p><p>Para que o modelo tenha interações realmente boas com um humano, a ponto de você — o humano — poder fazer uma pergunta ao chatbot e sua resposta parecer estruturalmente precisa, o LLM subjacente tem que passar por uma etapa de Aprendizado por Reforço com Feedback Humano . Este é literalmente o humano no loop que é frequentemente falado no contexto de modelos de Aprendizado de Máquina.</p><p>Nessa fase, os humanos marcam as previsões que não são tão boas e, ao receber esse feedback, os parâmetros do modelo são atualizados e o modelo é treinado novamente, quantas vezes forem necessárias, para atingir o nível de qualidade de previsão desejado.</p><p>Está claro agora que esses modelos são extremamente complexos e precisam ser capazes de executar milhões, se não bilhões de cálculos. Essa computação de alta intensidade exigiu novas arquiteturas, no nível do modelo com Transformers e para computação, com GPUs .</p><p>GPU é essa classe de processadores gráficos usada em cenários quando você precisa executar um número incrivelmente grande de cálculos em um curto período de tempo, por exemplo, ao renderizar suavemente personagens em um videogame. Comparadas às CPUs tradicionais encontradas em seu laptop ou PC de torre, as GPUs têm a capacidade de executar sem esforço muitos cálculos paralelos.</p><p>O avanço para LLMs foi quando pesquisadores perceberam que GPUs também podem ser aplicadas a problemas não gráficos. Tanto Machine Learning quanto Computer Graphics dependem de álgebra linear, executando operações em matrizes, então ambos se beneficiam da capacidade de executar muitas computações paralelas.</p><p>Transformers é um novo tipo de arquitetura desenvolvida pelo Google, que faz com que cada operação feita durante o treinamento do modelo possa ser paralelizada. Por exemplo, ao prever a próxima palavra em uma frase, um modelo que usa uma arquitetura Transformer não precisa ler a frase do início ao fim, ele processa o texto inteiro ao mesmo tempo, em paralelo. Ele associa cada palavra processada a uma longa matriz de números que dão significado a essa palavra. Pensando em Álgebra Linear novamente por um segundo, em vez de processar e transformar um ponto de dados por vez, a combinação de Transformers e GPUs pode processar toneladas de pontos ao mesmo tempo, aproveitando matrizes.</p><p>Além da computação paralelizada, o que distingue os Transformers é uma operação única chamada Atenção. De uma forma muito simplista, Atenção torna possível olhar todo o contexto em torno de uma palavra, mesmo que ela ocorra várias vezes em frases diferentes como</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>At the end of the show, the singer took a bow multiple times.</span></span></code></pre></div><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>Jack wanted to go to the store to buy a new bow for target practice.</span></span></code></pre></div><p>Se nos concentrarmos na palavra <code>bow</code>, você pode ver como o contexto em que essa palavra aparece em cada frase e seu significado real são muito diferentes.</p><p>A atenção permite que o modelo refine o significado que cada palavra codifica com base no contexto ao redor delas.</p><p>Isso, mais alguns passos adicionais como treinar uma Rede Neural Feedforward , tudo feito várias vezes, faz com que o modelo gradualmente refine sua capacidade de codificar as informações corretas. Todos esses passos têm a intenção de tornar o modelo mais preciso e não misturar o significado de <code>bow</code> , o movimento e <code>bow</code> (objeto relacionado ao arco e flecha) quando ele executa uma tarefa de previsão.</p><p>Imagem e legenda retiradas do artigo referenciado em [^2].</p><p><img src="'+c+'" alt="alt text"></p>',18)),t(s,null,{default:n(()=>e[2]||(e[2]=[a("small",null,"Um diagrama de fluxo básico representando vários estágios de LLMs do pré-treinamento ao prompt/utilização. Prompts LLMs para gerar respostas são possíveis em diferentes estágios de treinamento como pré-treinamento, ajuste de instrução ou ajuste de alinhamento. “RL” significa aprendizado por reforço, “RM” representa modelagem de recompensa e “RLHF” representa aprendizado por reforço com feedback humano. ",-1)])),_:1}),e[13]||(e[13]=r('<p>O desenvolvimento de Transformers e GPUs permitiu que os LLMs explodissem em uso e aplicação em comparação aos modelos de linguagem anteriores que precisavam ler uma palavra por vez. Sabendo que um modelo fica melhor quanto mais dados de qualidade ele aprende, você pode ver como processar uma palavra por vez era um grande gargalo.</p><h2 id="por-que-os-llms-sao-importantes" tabindex="-1">Por que os LLMs são importantes <a class="header-anchor" href="#por-que-os-llms-sao-importantes" aria-label="Permalink to &quot;Por que os LLMs são importantes&quot;">​</a></h2><p>Com a capacidade descrita, os LLMs podem processar enormes quantidades de exemplos de texto e então prever com alta precisão a próxima palavra em uma frase, combinado com outras estruturas poderosas de Inteligência Artificial, muitas tarefas de linguagem natural e recuperação de informações se tornaram muito mais fáceis de implementar e produtizar .</p><p>Em essência, os Grandes Modelos de Linguagem (LLMs) surgiram como sistemas de inteligência artificial de ponta que podem processar e gerar texto com comunicação coerente e generalizar múltiplas tarefas [2].</p><p>Pense em tarefas como traduzir do inglês para o espanhol, resumir um conjunto de documentos, identificar certas passagens em documentos ou ter um chatbot respondendo suas perguntas sobre um tópico específico.</p><p>Essas tarefas eram possíveis antes, mas o esforço necessário para construir um modelo era incrivelmente maior e a taxa de melhoria desses modelos era muito mais lenta devido a gargalos tecnológicos. Os LLMs chegaram e turbinaram todas essas tarefas e aplicações.</p><p>Você provavelmente já interagiu ou viu alguém interagindo diretamente com produtos que usam LLMs em sua essência.</p><p>Esses produtos são muito mais do que um simples LLM que prevê com precisão a próxima palavra em uma frase. Eles alavancam LLMs e outras técnicas e estruturas de Machine Learning para entender o que você está perguntando, pesquisar todas as informações contextuais que eles viram até agora e apresentar a você uma resposta humana e, na maioria das vezes, coerente. Ou pelo menos alguns fornecem orientação sobre o que procurar em seguida.</p><p>Existem vários produtos de Inteligência Artificial (IA) que alavancam LLMs, desde o Meta AI do Facebook , o Gemini do Google, o ChatGPT da Open AI , que toma emprestado seu nome da tecnologia Generative Pre-trained Transformer, o CoPilot da Microsoft , entre muitos, muitos outros, abrangendo uma ampla gama de tarefas para ajudar você.</p><h2 id="conclusao" tabindex="-1">Conclusão <a class="header-anchor" href="#conclusao" aria-label="Permalink to &quot;Conclusão&quot;">​</a></h2><p>Os LMMs são, sem dúvida, uma área de pesquisa emergente que vem evoluindo em um ritmo extremamente rápido, como você pode ver na linha do tempo abaixo.</p><p>Imagem e legenda retiradas do artigo referenciado em [^1]</p><p><img src="'+g+'" alt="alt text"></p>',13)),t(s,null,{default:n(()=>e[3]||(e[3]=[a("small",null,"Exibição cronológica de lançamentos de LLM: cartões azuis representam modelos 'pré-treinados', enquanto cartões laranja correspondem a modelos 'ajustados por instrução'. Os modelos na metade superior significam disponibilidade de código aberto, enquanto aqueles na parte inferior são de código fechado. O gráfico ilustra a tendência crescente em direção a modelos ajustados por instrução e de código aberto, destacando o cenário em evolução e as tendências na pesquisa de processamento de linguagem natural.",-1)])),_:1}),e[14]||(e[14]=a("p",null,"Estamos apenas nos primeiros dias da produtização, ou aplicação de produtos. Mais e mais empresas estão aplicando LLMs em suas áreas de domínio, a fim de simplificar tarefas que levariam vários anos e uma quantidade incrível de fundos para pesquisar, desenvolver e levar ao mercado.",-1)),e[15]||(e[15]=a("p",null,"Quando aplicados de forma ética e consciente do consumidor, os LLMs e produtos que têm LLMs em seu núcleo fornecem uma grande oportunidade para todos. Para pesquisadores, é um campo de ponta com uma riqueza de problemas teóricos e práticos para desembaraçar.",-1)),e[16]||(e[16]=a("p",null,"Por exemplo, em Genômica, gLMs ou Modelos de Linguagem Genômica, ou seja, Grandes Modelos de Linguagem treinados em sequências de DNA, são usados ​​para acelerar nossa compreensão geral dos genomas e como o DNA funciona e interage com outras funções[4]. Essas são grandes questões para as quais os cientistas não têm respostas definitivas, mas os LLMs estão provando ser uma ferramenta que pode ajudá-los a progredir em uma escala muito maior e iterar suas descobertas muito mais rápido. Para fazer um progresso constante na ciência, ciclos de feedback rápidos são cruciais.",-1)),e[17]||(e[17]=a("h2",{id:"referencias",tabindex:"-1"},[o("Referências "),a("a",{class:"header-anchor",href:"#referencias","aria-label":'Permalink to "Referências"'},"​")],-1)),e[18]||(e[18]=a("p",null,"[^1]:Uma visão geral abrangente de grandes modelos de linguagem . 2024. Humza Naveed e Asad Ullah Khan e Shi Qiu e Muhammad Saqib e Saeed Anwar e Muhammad Usman e Naveed Akhtar e Nick Barnes e Ajmal Mian.",-1)),e[19]||(e[19]=a("p",null,"[3] Modelos de linguagem são aprendizes de poucas tentativas . 2020. Tom B. Brown e Benjamin Mann e Nick Ryder e Melanie Subbiah e Jared Kaplan e Prafulla Dhariwal e Arvind Neelakantan e Pranav Shyam e Girish Sastry e Amanda Askell e Sandhini Agarwal e Ariel Herbert-Voss e Gretchen Krueger e Tom Henighan e Rewon Child e Aditya Ramesh e Daniel M. Ziegler e Jeffrey Wu e Clemens Winter e Christopher Hesse e Mark Chen e Eric Sigler e Mateusz Litwin e Scott Gray e Benjamin Chess e Jack Clark e Christopher Berner e Sam McCandlish e Alec Radford e Ilya Sutskever e Dario Amodei",-1)),e[20]||(e[20]=a("p",null,"[4] Modelos de linguagem genômica: oportunidades e desafios . 2024. Gonzalo Benegas e Chengzhong Ye e Carlos Albors e Jianan Canal Li e Yun S. Song.",-1)),e[21]||(e[21]=a("p",null,'[^2]: Bento, Carolina. "Large Language Models: A Short Introduction." Medium, Towards Data Science, 2023.',-1))])}const z=i(f,[["render",v]]);export{k as __pageData,z as default};
